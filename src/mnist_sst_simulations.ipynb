{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc1cb82",
   "metadata": {},
   "source": [
    "# MNIST and SST-2 Conformal Changepoint Localization\n",
    "\n",
    "This notebook implements changepoint detection on MNIST digit images and SST-2 sentiment changes using conformal prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, ks_1samp, uniform\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a61947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\n",
    "    \"svg\"\n",
    ")\n",
    "plt.style.use(\"math.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca66c26",
   "metadata": {},
   "source": [
    "## MNIST Dataset with Changepoint\n",
    "\n",
    "Create a dataset with digit 3 before the changepoint and digit 7 after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a23210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_model(device=\"cpu\"):\n",
    "    model = torch.hub.load(\n",
    "        \"pytorch/vision:v0.10.0\", \"resnet18\", weights=ResNet18_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "    )\n",
    "\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnist_dataset(length, changepoint, digit1=3, digit2=7):\n",
    "    \"\"\"Generate MNIST dataset with a changepoint.\"\"\"\n",
    "    transform = transforms.ToTensor()\n",
    "    mnist_data = MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    data = mnist_data.data.numpy()\n",
    "    targets = mnist_data.targets.numpy()\n",
    "    \n",
    "    images_digit1 = data[targets == digit1]\n",
    "    images_digit2 = data[targets == digit2]\n",
    "    np.random.shuffle(images_digit1)\n",
    "    np.random.shuffle(images_digit2)\n",
    "    \n",
    "    n1 = changepoint + 1\n",
    "    n2 = length - n1\n",
    "    if n1 > len(images_digit1) or n2 > len(images_digit2):\n",
    "        raise ValueError(\"Insufficient images for the specified digits and length.\")\n",
    "        \n",
    "    data1 = images_digit1[:n1]\n",
    "    data2 = images_digit2[:n2]\n",
    "    x = np.concatenate([data1, data2], axis=0)\n",
    "    \n",
    "    x = x.reshape(length, -1).astype(np.float32) / 255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bdafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_digit(model, image, device=\"cpu\"):\n",
    "    image = image.reshape(1, 1, 28, 28)\n",
    "    image_tensor = torch.tensor(image, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = torch.softmax(model(image_tensor), dim=1).cpu()\n",
    "        predicted = outputs.argmax(dim=1).item()\n",
    "    return (predicted, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_trained_model(device=\"cpu\"):\n",
    "    class MNISTModel(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MNISTModel, self).__init__()\n",
    "            self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = torch.nn.Dropout(0.25)\n",
    "            self.dropout2 = torch.nn.Dropout(0.5)\n",
    "            self.fc1 = torch.nn.Linear(9216, 128)\n",
    "            self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = torch.nn.functional.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    model = MNISTModel().to(device)\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    train_dataset = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Training MNIST model...\")\n",
    "    model.train()\n",
    "    for epoch in range(\n",
    "        1\n",
    "    ):\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)} \"\n",
    "                    f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "                )\n",
    "\n",
    "    test_dataset = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    print(f\"Test accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195be2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "changepoint = 400\n",
    "digit1 = 3\n",
    "digit2 = 7\n",
    "\n",
    "x = generate_mnist_dataset(length, changepoint, digit1, digit2)\n",
    "\n",
    "predicted_digits = [predict_digit(model, x[i]) for i in tqdm(range(length))]\n",
    "probabilities = torch.vstack([prob for _, prob in predicted_digits])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a8425",
   "metadata": {},
   "source": [
    "## Compute Left and Right Scores\n",
    "\n",
    "These scores are used for the conformal changepoint analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab487af",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_score = np.zeros((length, length))\n",
    "seen_digits = {}\n",
    "for t, (predicted, _) in enumerate(predicted_digits):\n",
    "    if predicted in seen_digits:\n",
    "        seen_digits[predicted] += 1\n",
    "    else:\n",
    "        seen_digits[predicted] = 1\n",
    "    curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "    left_score[t, : t + 1] = probabilities[: t + 1, curr_digit].cpu() / (\n",
    "        1 - probabilities[: t + 1, curr_digit].cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c84e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_score = np.zeros((length, length))\n",
    "seen_digits = {}\n",
    "for i, (predicted, _) in enumerate(reversed(predicted_digits)):\n",
    "    t = length - i - 1\n",
    "    if predicted in seen_digits:\n",
    "        seen_digits[predicted] += 1\n",
    "    else:\n",
    "        seen_digits[predicted] = 1\n",
    "    curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "    right_score[t, t:] = probabilities[t:, curr_digit].cpu() / (\n",
    "        1 - probabilities[t:, curr_digit].cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9834895",
   "metadata": {},
   "source": [
    "## Calculate Discrepancy Scores\n",
    "\n",
    "Using KS test to detect the changepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrepancy_scores(x, scores_left, scores_right):\n",
    "    discrepancy_scores = np.empty(len(x) - 1)\n",
    "    statistics = []\n",
    "    for t in tqdm(range(len(x) - 1)):\n",
    "        p = np.empty(len(x))\n",
    "        for r in range(t + 1):\n",
    "            p[r] = (\n",
    "                np.count_nonzero(scores_left[t, : r + 1] > scores_left[t, r])\n",
    "                + np.random.uniform(0, 1) * np.count_nonzero(scores_left[t, : r + 1] == scores_left[t, r])\n",
    "            ) / (r + 1)\n",
    "        for r in range(len(x) - 1, t, -1):\n",
    "            p[r] = (\n",
    "                np.count_nonzero(scores_right[t, r:] > scores_right[t, r])\n",
    "                + np.random.uniform(0, 1) * np.count_nonzero(scores_right[t, r:] == scores_right[t, r])\n",
    "            ) / (len(x) - r)\n",
    "        statistics.append((ks_1samp(p[: t + 1], uniform.cdf), ks_1samp(p[t+1:], uniform.cdf)))\n",
    "        discrepancy_scores[t] = (\n",
    "            statistics[-1][0].statistic * np.sqrt(t + 1)\n",
    "            + statistics[-1][1].statistic * np.sqrt(len(x) - t - 1)\n",
    "        )\n",
    "    return discrepancy_scores, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrepancy_scores, statistics = get_discrepancy_scores(x, left_score, right_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(discrepancy_scores)\n",
    "plt.axvline(x=changepoint, color='red', linestyle='--', label='True Changepoint')\n",
    "plt.xlabel('Position t')\n",
    "plt.ylabel('Discrepancy Score')\n",
    "plt.title('MNIST Changepoint Detection')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305775a",
   "metadata": {},
   "source": [
    "## Calculate p-values\n",
    "\n",
    "Using the Bonferroni correction to combine p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "p_values_left = np.array([s[0].pvalue for s in statistics])\n",
    "p_values_right = np.array([s[1].pvalue for s in statistics])\n",
    "\n",
    "min_method = chi2.cdf(np.minimum(2 * p_values_left, 2 * p_values_right, np.ones_like(p_values_left)), 4)\n",
    "threshold = 0.05\n",
    "\n",
    "plt.plot(np.arange(1, length), min_method)\n",
    "plt.axvline(\n",
    "    changepoint, color=\"red\", linestyle=\"--\", label=\"Changepoint ($\\\\xi = 400$)\"\n",
    ")\n",
    "plt.axhline(threshold, color='green', linestyle=':', label='Threshold ($\\\\alpha = 0.05$)')\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"p-value ($p_t$)\")\n",
    "plt.title(\"p-values for MNIST digit change (pre-trained classifier)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"images/mnist-pvalues.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_set = np.argwhere(min_method > threshold).flatten()\n",
    "confidence_interval = (confidence_set[0], confidence_set[-1]) if len(confidence_set) > 0 else None\n",
    "\n",
    "print(f\"True changepoint: {changepoint}\")\n",
    "print(f\"Confidence interval: {confidence_interval}\")\n",
    "print(f\"Minimum of Fisher's statistic at t={np.argmax(min_method)+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_confidence_intervals(\n",
    "    n_trials=50,\n",
    "    length=1000,\n",
    "    changepoint=400,\n",
    "    alpha=0.05,\n",
    "    digit1=3,\n",
    "    digit2=7,\n",
    "    calibration_size=100,\n",
    "    method=\"basic\",\n",
    "):\n",
    "    coverages = []\n",
    "    widths = []\n",
    "\n",
    "    for trial in tqdm(range(n_trials), desc=f\"Running {method} method trials\"):\n",
    "        if method == \"basic\":\n",
    "            x = generate_mnist_dataset(length, changepoint, digit1, digit2)\n",
    "            predicted_digits = [predict_digit(model, x[i]) for i in range(length)]\n",
    "            probabilities = torch.vstack([prob for _, prob in predicted_digits])\n",
    "\n",
    "            left_score = np.zeros((length, length))\n",
    "            seen_digits = {}\n",
    "            for t, (predicted, _) in enumerate(predicted_digits):\n",
    "                if predicted in seen_digits:\n",
    "                    seen_digits[predicted] += 1\n",
    "                else:\n",
    "                    seen_digits[predicted] = 1\n",
    "                curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "                left_score[t, : t + 1] = probabilities[: t + 1, curr_digit].cpu() / (\n",
    "                    1 - probabilities[: t + 1, curr_digit].cpu()\n",
    "                )\n",
    "\n",
    "            right_score = np.zeros((length, length))\n",
    "            seen_digits = {}\n",
    "            for i, (predicted, _) in enumerate(reversed(predicted_digits)):\n",
    "                t = length - i - 1\n",
    "                if predicted in seen_digits:\n",
    "                    seen_digits[predicted] += 1\n",
    "                else:\n",
    "                    seen_digits[predicted] = 1\n",
    "                curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "                right_score[t, t:] = probabilities[t:, curr_digit].cpu() / (\n",
    "                    1 - probabilities[t:, curr_digit].cpu()\n",
    "                )\n",
    "\n",
    "            discrepancy_scores, statistics = get_discrepancy_scores(\n",
    "                x, left_score, right_score\n",
    "            )\n",
    "\n",
    "        elif method == \"both_cal\":\n",
    "            x_main, x_cal_pre, x_cal_post = generate_extended_mnist_dataset(\n",
    "                length, changepoint, calibration_size, digit1, digit2\n",
    "            )\n",
    "\n",
    "            predicted_digits = [predict_digit(model, x_main[i]) for i in range(length)]\n",
    "            probabilities = torch.vstack([prob for _, prob in predicted_digits])\n",
    "\n",
    "            predicted_cal_pre = [\n",
    "                predict_digit(model, x_cal_pre[i]) for i in range(calibration_size)\n",
    "            ]\n",
    "            probabilities_cal_pre = torch.vstack(\n",
    "                [prob for _, prob in predicted_cal_pre]\n",
    "            )\n",
    "\n",
    "            predicted_cal_post = [\n",
    "                predict_digit(model, x_cal_post[i]) for i in range(calibration_size)\n",
    "            ]\n",
    "            probabilities_cal_post = torch.vstack(\n",
    "                [prob for _, prob in predicted_cal_post]\n",
    "            )\n",
    "\n",
    "            left_scores, left_scores_cal = compute_left_scores_with_calibration(\n",
    "                probabilities,\n",
    "                predicted_digits,\n",
    "                probabilities_cal_pre,\n",
    "                predicted_cal_pre,\n",
    "                length,\n",
    "            )\n",
    "\n",
    "            right_scores, right_scores_cal = compute_right_scores_with_calibration(\n",
    "                probabilities,\n",
    "                predicted_digits,\n",
    "                probabilities_cal_post,\n",
    "                predicted_cal_post,\n",
    "                length,\n",
    "            )\n",
    "\n",
    "            # Calculate discrepancy scores with calibration\n",
    "            discrepancy_scores, statistics = get_discrepancy_scores_with_calibration(\n",
    "                x_main, left_scores, right_scores, left_scores_cal, right_scores_cal\n",
    "            )\n",
    "\n",
    "        elif method == \"left_cal\":\n",
    "            x_main, x_cal_pre = generate_left_calibration_mnist_dataset(\n",
    "                length, changepoint, calibration_size, digit1, digit2\n",
    "            )\n",
    "\n",
    "            # Get predictions\n",
    "            predicted_digits = [predict_digit(model, x_main[i]) for i in range(length)]\n",
    "            probabilities = torch.vstack([prob for _, prob in predicted_digits])\n",
    "\n",
    "            predicted_cal_pre = [\n",
    "                predict_digit(model, x_cal_pre[i]) for i in range(calibration_size)\n",
    "            ]\n",
    "            probabilities_cal_pre = torch.vstack(\n",
    "                [prob for _, prob in predicted_cal_pre]\n",
    "            )\n",
    "\n",
    "            # Compute scores\n",
    "            left_scores, left_scores_cal = compute_left_scores_with_left_calibration(\n",
    "                probabilities,\n",
    "                predicted_digits,\n",
    "                probabilities_cal_pre,\n",
    "                predicted_cal_pre,\n",
    "                length,\n",
    "            )\n",
    "\n",
    "            right_scores = compute_right_scores_without_calibration(\n",
    "                probabilities, predicted_digits, length\n",
    "            )\n",
    "\n",
    "            # Calculate discrepancy scores\n",
    "            discrepancy_scores, statistics = (\n",
    "                get_discrepancy_scores_with_left_calibration(\n",
    "                    x_main, left_scores, right_scores, left_scores_cal\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Extract p-values\n",
    "        p_values_left = np.array([s[0].pvalue for s in statistics])\n",
    "        p_values_right = np.array([s[1].pvalue for s in statistics])\n",
    "\n",
    "        # Fisher's method to combine p-values\n",
    "        min_method = chi2.cdf(\n",
    "            np.minimum(\n",
    "                2 * p_values_left, 2 * p_values_right, np.ones_like(p_values_left)\n",
    "            ),\n",
    "            4,\n",
    "        )\n",
    "\n",
    "        # Get confidence set\n",
    "        confidence_set = np.argwhere(min_method <= alpha).flatten()\n",
    "\n",
    "        if len(confidence_set) > 0:\n",
    "            # Calculate interval width\n",
    "            width = confidence_set[-1] - confidence_set[0]\n",
    "            widths.append(width)\n",
    "\n",
    "            # Check coverage\n",
    "            coverage = changepoint + 1 in confidence_set\n",
    "            coverages.append(coverage)\n",
    "        else:\n",
    "            # Empty confidence set - consider as no coverage\n",
    "            coverages.append(False)\n",
    "            widths.append(0)  # Width is 0 for empty set\n",
    "\n",
    "    # Compute overall statistics\n",
    "    coverage_rate = np.mean(coverages)\n",
    "    avg_width = np.mean(widths)\n",
    "\n",
    "    return coverage_rate, avg_width\n",
    "\n",
    "\n",
    "# Function to compare all methods\n",
    "def compare_methods(\n",
    "    n_trials=20,\n",
    "    length=200,\n",
    "    changepoint=80,\n",
    "    alpha=0.05,\n",
    "    digit1=3,\n",
    "    digit2=7,\n",
    "    calibration_size=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare different changepoint detection methods.\n",
    "    \"\"\"\n",
    "    methods = [\"basic\"]\n",
    "    results = {}\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"Evaluating {method} method...\")\n",
    "        coverage, avg_width = evaluate_confidence_intervals(\n",
    "            n_trials=n_trials,\n",
    "            length=length,\n",
    "            changepoint=changepoint,\n",
    "            alpha=alpha,\n",
    "            digit1=digit1,\n",
    "            digit2=digit2,\n",
    "            calibration_size=calibration_size,\n",
    "            method=method,\n",
    "        )\n",
    "        results[method] = {\"coverage\": coverage, \"avg_width\": avg_width}\n",
    "        print(f\"{method} - Coverage: {coverage:.4f}, Avg Width: {avg_width:.2f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"model\" not in locals():\n",
    "    model = get_mnist_trained_model()\n",
    "\n",
    "results = compare_methods(n_trials=200)\n",
    "\n",
    "methods = list(results.keys())\n",
    "coverages = [results[m][\"coverage\"] for m in methods]\n",
    "widths = [results[m][\"avg_width\"] for m in methods]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width / 2, coverages, width, label=\"Coverage\", color=\"blue\", alpha=0.7)\n",
    "ax1.set_ylabel(\"Coverage\", color=\"blue\")\n",
    "ax1.set_ylim([0, 1.1])\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + width / 2, widths, width, label=\"Average Width\", color=\"red\", alpha=0.7)\n",
    "ax2.set_ylabel(\"Average Width\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "plt.xticks(x, methods)\n",
    "plt.title(\"Coverage and Average Width Comparison\")\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f779b63",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "Display some example images before and after the changepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11922626",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_mnist_dataset(length, changepoint, digit1, digit2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(6, 5))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = changepoint - 2 + i\n",
    "    img = x[idx].reshape(28, 28)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    if i != 2:\n",
    "        ax.set_title(f\"$t = {idx}$\")\n",
    "    else:\n",
    "        ax.set_title(f\"$t = \\\\xi = {idx}$\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/mnist-sample.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a209391",
   "metadata": {},
   "source": [
    "# Augment on both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_extended_mnist_dataset(\n",
    "    length, changepoint, calibration_size=100, digit1=3, digit2=7\n",
    "):\n",
    "    \"\"\"Generate MNIST dataset with a changepoint and calibration data.\"\"\"\n",
    "    transform = transforms.ToTensor()\n",
    "    mnist_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    data = mnist_data.data.numpy()\n",
    "    targets = mnist_data.targets.numpy()\n",
    "\n",
    "    images_digit1 = data[targets == digit1]\n",
    "    images_digit2 = data[targets == digit2]\n",
    "    np.random.shuffle(images_digit1)\n",
    "    np.random.shuffle(images_digit2)\n",
    "\n",
    "    n1 = changepoint + 1 + calibration_size\n",
    "    n2 = (length - changepoint - 1) + calibration_size\n",
    "\n",
    "    if n1 > len(images_digit1) or n2 > len(images_digit2):\n",
    "        raise ValueError(\"Insufficient images for the specified digits and length.\")\n",
    "\n",
    "    data1 = images_digit1[:n1]\n",
    "    data2 = images_digit2[:n2]\n",
    "\n",
    "    calibration_pre = data1[:calibration_size]\n",
    "    main_pre = data1[calibration_size:n1]\n",
    "    calibration_post = data2[:calibration_size]\n",
    "    main_post = data2[calibration_size:n2]\n",
    "\n",
    "    x_main = np.concatenate([main_pre, main_post], axis=0)\n",
    "    x_calibration_pre = calibration_pre\n",
    "    x_calibration_post = calibration_post\n",
    "\n",
    "    x_main = x_main.reshape(length, -1).astype(np.float32) / 255.0\n",
    "    x_calibration_pre = (\n",
    "        x_calibration_pre.reshape(calibration_size, -1).astype(np.float32) / 255.0\n",
    "    )\n",
    "    x_calibration_post = (\n",
    "        x_calibration_post.reshape(calibration_size, -1).astype(np.float32) / 255.0\n",
    "    )\n",
    "\n",
    "    return x_main, x_calibration_pre, x_calibration_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c699c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "changepoint = 400\n",
    "calibration_size = 100\n",
    "digit1 = 3\n",
    "digit2 = 7\n",
    "\n",
    "x_main, x_cal_pre, x_cal_post = generate_extended_mnist_dataset(\n",
    "    length, changepoint, calibration_size, digit1, digit2\n",
    ")\n",
    "\n",
    "predicted_digits = [predict_digit(model, x_main[i]) for i in tqdm(range(length))]\n",
    "probabilities = torch.vstack([prob for _, prob in predicted_digits])\n",
    "\n",
    "predicted_cal_pre = [\n",
    "    predict_digit(model, x_cal_pre[i]) for i in tqdm(range(calibration_size))\n",
    "]\n",
    "probabilities_cal_pre = torch.vstack([prob for _, prob in predicted_cal_pre])\n",
    "\n",
    "predicted_cal_post = [\n",
    "    predict_digit(model, x_cal_post[i]) for i in tqdm(range(calibration_size))\n",
    "]\n",
    "probabilities_cal_post = torch.vstack([prob for _, prob in predicted_cal_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_left_scores_with_calibration(\n",
    "    probabilities, predicted_digits, probabilities_cal_pre, predicted_cal_pre, length\n",
    "):\n",
    "    left_scores = np.zeros((length, length))\n",
    "    left_scores_cal = np.zeros((length, calibration_size))\n",
    "\n",
    "    seen_digits = {}\n",
    "\n",
    "    for i, (predicted, _) in enumerate(predicted_cal_pre):\n",
    "        if predicted in seen_digits:\n",
    "            seen_digits[predicted] += 1\n",
    "        else:\n",
    "            seen_digits[predicted] = 1\n",
    "\n",
    "    for t, (predicted, _) in enumerate(predicted_digits):\n",
    "        if predicted in seen_digits:\n",
    "            seen_digits[predicted] += 1\n",
    "        else:\n",
    "            seen_digits[predicted] = 1\n",
    "        curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "\n",
    "        left_scores[t, : t + 1] = probabilities[: t + 1, curr_digit].cpu() / (\n",
    "            1 - probabilities[: t + 1, curr_digit].cpu()\n",
    "        )\n",
    "\n",
    "        left_scores_cal[t, :] = probabilities_cal_pre[:, curr_digit].cpu() / (\n",
    "            1 - probabilities_cal_pre[:, curr_digit].cpu()\n",
    "        )\n",
    "\n",
    "    return left_scores, left_scores_cal\n",
    "\n",
    "\n",
    "def compute_right_scores_with_calibration(\n",
    "    probabilities, predicted_digits, probabilities_cal_post, predicted_cal_post, length\n",
    "):\n",
    "    calibration_size = len(\n",
    "        predicted_cal_post\n",
    "    )\n",
    "    right_scores = np.zeros((length, length))\n",
    "    right_scores_cal = np.zeros((length, calibration_size))\n",
    "\n",
    "    seen_digits = {}\n",
    "\n",
    "    for i, (predicted, _) in enumerate(predicted_cal_post):\n",
    "        if predicted in seen_digits:\n",
    "            seen_digits[predicted] += 1\n",
    "        else:\n",
    "            seen_digits[predicted] = 1\n",
    "\n",
    "    for i, (predicted, _) in enumerate(reversed(predicted_digits)):\n",
    "        t = length - i - 1\n",
    "        if predicted in seen_digits:\n",
    "            seen_digits[predicted] += 1\n",
    "        else:\n",
    "            seen_digits[predicted] = 1\n",
    "        curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "\n",
    "        right_scores[t, t:] = probabilities[t:, curr_digit].cpu() / (\n",
    "            1 - probabilities[t:, curr_digit].cpu()\n",
    "        )\n",
    "\n",
    "        right_scores_cal[t, :] = probabilities_cal_post[:, curr_digit].cpu() / (\n",
    "            1 - probabilities_cal_post[:, curr_digit].cpu()\n",
    "        )\n",
    "\n",
    "    return right_scores, right_scores_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrepancy_scores_with_calibration(\n",
    "    x, scores_left, scores_right, scores_left_cal, scores_right_cal\n",
    "):\n",
    "    n = len(x)\n",
    "    calibration_size_pre = scores_left_cal.shape[1]\n",
    "    calibration_size_post = scores_right_cal.shape[1]\n",
    "    discrepancy_scores = np.empty(n - 1)\n",
    "    statistics = []\n",
    "\n",
    "    for t in tqdm(range(n - 1)):\n",
    "        p = np.empty(n)\n",
    "\n",
    "        for r in range(t + 1):\n",
    "            score_r = scores_left[t, r]\n",
    "\n",
    "            main_counts = np.sum(scores_left[t, : r + 1] < score_r) + np.random.uniform(\n",
    "                0, 1\n",
    "            ) * np.sum(scores_left[t, : r + 1] == score_r)\n",
    "\n",
    "            cal_counts = np.sum(scores_left_cal[t, :] < score_r) + np.random.uniform(\n",
    "                0, 1\n",
    "            ) * np.sum(scores_left_cal[t, :] == score_r)\n",
    "\n",
    "            p[r] = (main_counts + cal_counts) / (r + 1 + calibration_size_pre)\n",
    "\n",
    "        for r in range(n - 1, t, -1):\n",
    "            score_r = scores_right[t, r]\n",
    "\n",
    "            main_counts = np.sum(scores_right[t, r:] < score_r) + np.random.uniform(\n",
    "                0, 1\n",
    "            ) * np.sum(scores_right[t, r:] == score_r)\n",
    "\n",
    "            cal_counts = np.sum(scores_right_cal[t, :] < score_r) + np.random.uniform(\n",
    "                0, 1\n",
    "            ) * np.sum(scores_right_cal[t, :] == score_r)\n",
    "\n",
    "            p[r] = (main_counts + cal_counts) / (n - r + calibration_size_post)\n",
    "\n",
    "        statistics.append(\n",
    "            (ks_1samp(p[: t + 1], uniform.cdf), ks_1samp(p[t + 1 :], uniform.cdf))\n",
    "        )\n",
    "        discrepancy_scores[t] = statistics[-1][0].statistic * np.sqrt(\n",
    "            t + 1\n",
    "        ) + statistics[-1][1].statistic * np.sqrt(n - t - 1)\n",
    "\n",
    "    return discrepancy_scores, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_scores, left_scores_cal = compute_left_scores_with_calibration(\n",
    "    probabilities, predicted_digits, probabilities_cal_pre, predicted_cal_pre, length\n",
    ")\n",
    "\n",
    "right_scores, right_scores_cal = compute_right_scores_with_calibration(\n",
    "    probabilities, predicted_digits, probabilities_cal_post, predicted_cal_post, length\n",
    ")\n",
    "\n",
    "discrepancy_scores_cal, statistics_cal = get_discrepancy_scores_with_calibration(\n",
    "    x_main, left_scores, right_scores, left_scores_cal, right_scores_cal\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(discrepancy_scores_cal)\n",
    "plt.axvline(x=changepoint, color=\"red\", linestyle=\"--\", label=\"True Changepoint\")\n",
    "plt.xlabel(\"Position t\")\n",
    "plt.ylabel(\"Discrepancy Score\")\n",
    "plt.title(\"MNIST Changepoint Detection with Calibration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "p_values_left = np.array([s[0].pvalue for s in statistics_cal])\n",
    "p_values_right = np.array([s[1].pvalue for s in statistics_cal])\n",
    "\n",
    "min_method = chi2.cdf(\n",
    "    np.minimum(2 * p_values_left, 2 * p_values_right, np.ones_like(p_values_left)), 4\n",
    ")\n",
    "threshold = 0.05\n",
    "\n",
    "plt.plot(np.arange(1, length), min_method)\n",
    "plt.axvline(\n",
    "    changepoint, color=\"red\", linestyle=\"--\", label=\"Changepoint ($\\\\xi = 400$)\"\n",
    ")\n",
    "plt.axhline(\n",
    "    threshold, color=\"green\", linestyle=\":\", label=\"Threshold ($\\\\alpha = 0.05$)\"\n",
    ")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"p-value ($p_t$)\")\n",
    "plt.title(\"p-values for MNIST digit change (two-sided calibration)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"images/mnist-pvalues-calibrated.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2303a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = np.argwhere(min_method > threshold).flatten()\n",
    "confidence_interval = (ci[0], ci[-1]) if len(ci) > 0 else None\n",
    "print(f\"True changepoint: {changepoint}\")\n",
    "print(f\"Confidence interval: {confidence_interval}\")\n",
    "print(f\"Minimum of Fisher's statistic at t={np.argmax(min_method)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62e19f",
   "metadata": {},
   "source": [
    "# Left side only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a100a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_left_calibration_mnist_dataset(\n",
    "    length, changepoint, calibration_size=100, digit1=3, digit2=7\n",
    "):\n",
    "    transform = transforms.ToTensor()\n",
    "    mnist_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    data = mnist_data.data.numpy()\n",
    "    targets = mnist_data.targets.numpy()\n",
    "\n",
    "    images_digit1 = data[targets == digit1]\n",
    "    images_digit2 = data[targets == digit2]\n",
    "    np.random.shuffle(images_digit1)\n",
    "    np.random.shuffle(images_digit2)\n",
    "\n",
    "    n1 = changepoint + 1 + calibration_size\n",
    "    n2 = length - changepoint - 1\n",
    "\n",
    "    if n1 > len(images_digit1) or n2 > len(images_digit2):\n",
    "        raise ValueError(\"Insufficient images for the specified digits and length.\")\n",
    "\n",
    "    data1 = images_digit1[:n1]\n",
    "    data2 = images_digit2[:n2]\n",
    "\n",
    "    calibration_pre = data1[:calibration_size]\n",
    "    main_pre = data1[calibration_size:n1]\n",
    "    main_post = data2\n",
    "\n",
    "    x_main = np.concatenate([main_pre, main_post], axis=0)\n",
    "    x_calibration_pre = calibration_pre\n",
    "\n",
    "    x_main = x_main.reshape(length, -1).astype(np.float32) / 255.0\n",
    "    x_calibration_pre = (\n",
    "        x_calibration_pre.reshape(calibration_size, -1).astype(np.float32) / 255.0\n",
    "    )\n",
    "\n",
    "    return x_main, x_calibration_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_left_scores_with_left_calibration(\n",
    "    probabilities, predicted_digits, probabilities_cal_pre, predicted_cal_pre, length\n",
    "):\n",
    "    left_scores = np.zeros((length, length))\n",
    "    left_scores_cal = np.zeros((length, len(predicted_cal_pre)))\n",
    "\n",
    "    seen_digits = {}\n",
    "\n",
    "    for i, (predicted, _) in enumerate(predicted_cal_pre):\n",
    "        if predicted in seen_digits:\n",
    "            seen_digits[predicted] += 1\n",
    "        else:\n",
    "            seen_digits[predicted] = 1\n",
    "\n",
    "    for t, (predicted, _) in enumerate(predicted_digits):\n",
    "        if predicted in seen_digits:\n",
    "            seen_digits[predicted] += 1\n",
    "        else:\n",
    "            seen_digits[predicted] = 1\n",
    "        curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "\n",
    "        left_scores[t, : t + 1] = probabilities[: t + 1, curr_digit].cpu() / (\n",
    "            1 - probabilities[: t + 1, curr_digit].cpu()\n",
    "        )\n",
    "\n",
    "        left_scores_cal[t, :] = probabilities_cal_pre[:, curr_digit].cpu() / (\n",
    "            1 - probabilities_cal_pre[:, curr_digit].cpu()\n",
    "        )\n",
    "\n",
    "    return left_scores, left_scores_cal\n",
    "\n",
    "\n",
    "def compute_right_scores_without_calibration(probabilities, predicted_digits, length):\n",
    "    right_scores = np.zeros((length, length))\n",
    "\n",
    "    seen_digits = {}\n",
    "\n",
    "    for i, (predicted, _) in enumerate(reversed(predicted_digits)):\n",
    "        t = length - i - 1\n",
    "        if predicted in seen_digits:\n",
    "            seen_digits[predicted] += 1\n",
    "        else:\n",
    "            seen_digits[predicted] = 1\n",
    "        curr_digit = max(seen_digits, key=seen_digits.get)\n",
    "\n",
    "        right_scores[t, t:] = probabilities[t:, curr_digit].cpu() / (\n",
    "            1 - probabilities[t:, curr_digit].cpu()\n",
    "        )\n",
    "\n",
    "    return right_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrepancy_scores_with_left_calibration(\n",
    "    x, scores_left, scores_right, scores_left_cal\n",
    "):\n",
    "    n = len(x)\n",
    "    calibration_size_pre = scores_left_cal.shape[1]\n",
    "    discrepancy_scores = np.empty(n - 1)\n",
    "    statistics = []\n",
    "\n",
    "    for t in tqdm(range(n - 1)):\n",
    "        p = np.empty(n)\n",
    "\n",
    "        for r in range(t + 1):\n",
    "            score_r = scores_left[t, r]\n",
    "\n",
    "            main_counts = np.sum(scores_left[t, : r + 1] < score_r) + np.random.uniform(\n",
    "                0, 1\n",
    "            ) * np.sum(scores_left[t, : r + 1] == score_r)\n",
    "\n",
    "            cal_counts = np.sum(scores_left_cal[t, :] < score_r) + np.random.uniform(\n",
    "                0, 1\n",
    "            ) * np.sum(scores_left_cal[t, :] == score_r)\n",
    "\n",
    "            p[r] = (main_counts + cal_counts) / (r + 1 + calibration_size_pre)\n",
    "\n",
    "        for r in range(n - 1, t, -1):\n",
    "            p[r] = (\n",
    "                np.count_nonzero(scores_right[t, r:] > scores_right[t, r])\n",
    "                + np.random.uniform(0, 1)\n",
    "                * np.count_nonzero(scores_right[t, r:] == scores_right[t, r])\n",
    "            ) / (n - r)\n",
    "\n",
    "        statistics.append(\n",
    "            (ks_1samp(p[: t + 1], uniform.cdf), ks_1samp(p[t + 1 :], uniform.cdf))\n",
    "        )\n",
    "        discrepancy_scores[t] = statistics[-1][0].statistic * np.sqrt(\n",
    "            t + 1\n",
    "        ) + statistics[-1][1].statistic * np.sqrt(n - t - 1)\n",
    "\n",
    "    return discrepancy_scores, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "changepoint = 400\n",
    "calibration_size = 100\n",
    "digit1 = 3\n",
    "digit2 = 7\n",
    "\n",
    "x_main, x_cal_pre = generate_left_calibration_mnist_dataset(\n",
    "    length, changepoint, calibration_size, digit1, digit2\n",
    ")\n",
    "\n",
    "predicted_digits = [predict_digit(model, x_main[i]) for i in tqdm(range(length))]\n",
    "probabilities = torch.vstack([prob for _, prob in predicted_digits])\n",
    "\n",
    "predicted_cal_pre = [\n",
    "    predict_digit(model, x_cal_pre[i]) for i in tqdm(range(calibration_size))\n",
    "]\n",
    "probabilities_cal_pre = torch.vstack([prob for _, prob in predicted_cal_pre])\n",
    "\n",
    "left_scores, left_scores_cal = compute_left_scores_with_left_calibration(\n",
    "    probabilities, predicted_digits, probabilities_cal_pre, predicted_cal_pre, length\n",
    ")\n",
    "\n",
    "right_scores = compute_right_scores_without_calibration(\n",
    "    probabilities, predicted_digits, length\n",
    ")\n",
    "\n",
    "discrepancy_scores_left_cal, statistics_left_cal = (\n",
    "    get_discrepancy_scores_with_left_calibration(\n",
    "        x_main, left_scores, right_scores, left_scores_cal\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(discrepancy_scores_left_cal)\n",
    "plt.axvline(x=changepoint, color=\"red\", linestyle=\"--\", label=\"True Changepoint\")\n",
    "plt.xlabel(\"Position t\")\n",
    "plt.ylabel(\"Discrepancy Score\")\n",
    "plt.title(\"MNIST Changepoint Detection with Left Calibration Only\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "p_values_left = np.array([s[0].pvalue for s in statistics_left_cal])\n",
    "p_values_right = np.array([s[1].pvalue for s in statistics_left_cal])\n",
    "\n",
    "min_method = chi2.cdf(\n",
    "    np.minimum(2 * p_values_left, 2 * p_values_right, np.ones_like(p_values_left)), 4\n",
    ")\n",
    "threshold = 0.05\n",
    "\n",
    "plt.plot(np.arange(1, length), min_method)\n",
    "plt.axhline(\n",
    "    threshold, color=\"green\", linestyle=\":\", label=\"Threshold ($\\\\alpha = 0.05$)\"\n",
    ")\n",
    "plt.axvline(\n",
    "    changepoint, color=\"red\", linestyle=\"--\", label=\"Changepoint ($\\\\xi = 400$)\"\n",
    ")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"p-value ($p_t$)\")\n",
    "plt.title(\"p-values for MNIST digit change (left-side calibration)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"images/mnist-pvalues-left-calibrated.pdf\")\n",
    "plt.show()\n",
    "\n",
    "confidence_set = np.argwhere(min_method > threshold).flatten()\n",
    "confidence_interval = (\n",
    "    (confidence_set[0], confidence_set[-1]) if len(confidence_set) > 0 else None\n",
    ")\n",
    "\n",
    "print(f\"True changepoint: {changepoint}\")\n",
    "print(f\"Confidence interval: {confidence_interval}\")\n",
    "print(f\"Minimum of Fisher's statistic at t={np.argmax(min_method)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f34d8",
   "metadata": {},
   "source": [
    "# LLM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from scipy.stats import ks_2samp, ks_1samp, uniform\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "plt.style.use(\"math.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "plt.style.use(\"math.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_sentiment_model(device=\"cpu\"):\n",
    "    model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiment_dataset(length, changepoint, dataset_name=\"sst2\"):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    positive_texts = [item[\"sentence\"] for item in train_data if item[\"label\"] == 1]\n",
    "    negative_texts = [item[\"sentence\"] for item in train_data if item[\"label\"] == 0]\n",
    "\n",
    "    random.shuffle(positive_texts)\n",
    "    random.shuffle(negative_texts)\n",
    "\n",
    "    n1 = changepoint + 1\n",
    "    n2 = length - n1\n",
    "\n",
    "    if n1 > len(positive_texts) or n2 > len(negative_texts):\n",
    "        raise ValueError(\"Insufficient texts for the specified length and changepoint.\")\n",
    "\n",
    "    texts_before = positive_texts[:n1]\n",
    "    texts_after = negative_texts[:n2]\n",
    "\n",
    "    texts = texts_before + texts_after\n",
    "    true_labels = [1] * n1 + [0] * n2\n",
    "\n",
    "    return texts, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb909d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, text, device=\"cpu\"):\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1).cpu()\n",
    "        predicted = probs.argmax(dim=1).item()\n",
    "\n",
    "    return (predicted, probs.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe62943",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 500\n",
    "changepoint = 200\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, tokenizer = get_pretrained_sentiment_model(device)\n",
    "\n",
    "print(\"Generating sentiment dataset...\")\n",
    "texts, true_labels = generate_sentiment_dataset(length, changepoint)\n",
    "\n",
    "print(\"Getting predictions...\")\n",
    "predictions = []\n",
    "probabilities = []\n",
    "\n",
    "for i, text in enumerate(tqdm(texts)):\n",
    "    pred, prob = predict_sentiment(model, tokenizer, text, device)\n",
    "    predictions.append(pred)\n",
    "    probabilities.append(prob)\n",
    "\n",
    "probabilities = torch.stack(\n",
    "    probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ba784",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_score = np.zeros((length, length))\n",
    "seen_sentiments = {0: 0, 1: 0}\n",
    "\n",
    "for t in range(length):\n",
    "    seen_sentiments[predictions[t]] += 1\n",
    "    curr_sentiment = max(seen_sentiments, key=seen_sentiments.get)\n",
    "\n",
    "    for r in range(t + 1):\n",
    "        left_score[t, r] = probabilities[r, curr_sentiment] / (\n",
    "            1 - probabilities[r, curr_sentiment]\n",
    "        )\n",
    "\n",
    "right_score = np.zeros((length, length))\n",
    "seen_sentiments = {0: 0, 1: 0}\n",
    "\n",
    "for i in range(length - 1, -1, -1):\n",
    "    t = length - i - 1\n",
    "    seen_sentiments[predictions[i]] += 1\n",
    "    curr_sentiment = max(seen_sentiments, key=seen_sentiments.get)\n",
    "\n",
    "    right_score[t, t:] = probabilities[t:, curr_sentiment] / (\n",
    "        1 - probabilities[t:, curr_sentiment]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrepancy_scores(scores_left, scores_right, length):\n",
    "    discrepancy_scores = np.empty(length - 1)\n",
    "    statistics = []\n",
    "\n",
    "    for t in tqdm(range(length - 1)):\n",
    "        p = np.empty(length)\n",
    "\n",
    "        for r in range(t + 1):\n",
    "            p[r] = (\n",
    "                np.count_nonzero(scores_left[t, : r + 1] > scores_left[t, r])\n",
    "                + np.random.uniform(0, 1)\n",
    "                * np.count_nonzero(scores_left[t, : r + 1] == scores_left[t, r])\n",
    "            ) / (r + 1)\n",
    "\n",
    "        for r in range(length - 1, t, -1):\n",
    "            p[r] = (\n",
    "                np.count_nonzero(scores_right[t, r:] > scores_right[t, r])\n",
    "                + np.random.uniform(0, 1)\n",
    "                * np.count_nonzero(scores_right[t, r:] == scores_right[t, r])\n",
    "            ) / (length - r)\n",
    "\n",
    "        statistics.append(\n",
    "            (ks_1samp(p[: t + 1], uniform.cdf), ks_1samp(p[t + 1 :], uniform.cdf))\n",
    "        )\n",
    "        discrepancy_scores[t] = statistics[-1][0].statistic * np.sqrt(\n",
    "            t + 1\n",
    "        ) + statistics[-1][1].statistic * np.sqrt(length - t - 1)\n",
    "\n",
    "    return discrepancy_scores, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating discrepancy scores...\")\n",
    "discrepancy_scores, statistics = get_discrepancy_scores(left_score, right_score, length)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(discrepancy_scores)\n",
    "plt.axvline(x=changepoint, color=\"red\", linestyle=\"--\", label=\"True Changepoint\")\n",
    "plt.xlabel(\"Position t\")\n",
    "plt.ylabel(\"Discrepancy Score\")\n",
    "plt.title(\"Sentiment Analysis Changepoint Detection\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f70edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "p_values_left = np.array([s[0].pvalue for s in statistics])\n",
    "p_values_right = np.array([s[1].pvalue for s in statistics])\n",
    "\n",
    "min_method = chi2.cdf(\n",
    "    np.minimum(2 * p_values_left, 2 * p_values_right, np.ones_like(p_values_left)), 4\n",
    ")\n",
    "threshold = 0.05\n",
    "\n",
    "plt.plot(np.arange(1, length), min_method)\n",
    "plt.axvline(\n",
    "    changepoint,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Changepoint ($\\\\xi = {changepoint}$)\",\n",
    ")\n",
    "plt.axhline(\n",
    "    threshold,\n",
    "    color=\"green\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"Threshold ($\\\\alpha = {threshold}$)\",\n",
    ")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"p-value ($p_t$)\")\n",
    "plt.title(\"p-values for SST-2 sentiment change\")\n",
    "plt.legend()\n",
    "plt.savefig(\"images/sentiment-pvalues.pdf\")\n",
    "plt.show()\n",
    "\n",
    "confidence_set = np.argwhere(min_method > threshold).flatten()\n",
    "confidence_interval = (\n",
    "    (confidence_set[0], confidence_set[-1]) if len(confidence_set) > 0 else None\n",
    ")\n",
    "\n",
    "print(f\"True changepoint: {changepoint}\")\n",
    "print(f\"Confidence interval: {confidence_interval}\")\n",
    "print(f\"Minimum of Fisher's statistic at t={np.argmax(min_method)+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e855e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExamples before changepoint (positive):\")\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(0, changepoint)\n",
    "    print(f'Text {i+1}: \"{texts[idx]}\"')\n",
    "    print(\n",
    "        f\"True label: Positive, Predicted: {'Positive' if predictions[idx] == 1 else 'Negative'}\"\n",
    "    )\n",
    "    print(f\"Confidence: {probabilities[idx][predictions[idx]]:.4f}\\n\")\n",
    "\n",
    "print(\"\\nExamples after changepoint (negative):\")\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(changepoint + 1, length)\n",
    "    print(f'Text {i+1}: \"{texts[idx]}\"')\n",
    "    print(\n",
    "        f\"True label: Negative, Predicted: {'Positive' if predictions[idx] == 1 else 'Negative'}\"\n",
    "    )\n",
    "    print(f\"Confidence: {probabilities[idx][predictions[idx]]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixed_sentiment_dataset(length, changepoint, dataset_name=\"sst2\"):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    positive_texts = [item[\"sentence\"] for item in train_data if item[\"label\"] == 1]\n",
    "    negative_texts = [item[\"sentence\"] for item in train_data if item[\"label\"] == 0]\n",
    "\n",
    "    random.shuffle(positive_texts)\n",
    "    random.shuffle(negative_texts)\n",
    "\n",
    "    n_pre = changepoint + 1\n",
    "    n_post = length - n_pre\n",
    "\n",
    "    n_pos_pre = int(n_pre * 0.6)\n",
    "    n_neg_pre = n_pre - n_pos_pre\n",
    "\n",
    "    n_pos_post = int(n_post * 0.4)\n",
    "    n_neg_post = n_post - n_pos_post\n",
    "\n",
    "    if n_pos_pre + n_pos_post > len(positive_texts) or n_neg_pre + n_neg_post > len(\n",
    "        negative_texts\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Insufficient texts for the specified distribution and length.\"\n",
    "        )\n",
    "\n",
    "    pre_pos_texts = positive_texts[:n_pos_pre]\n",
    "    pre_neg_texts = negative_texts[:n_neg_pre]\n",
    "    pre_texts = pre_pos_texts + pre_neg_texts\n",
    "    pre_labels = [1] * n_pos_pre + [0] * n_neg_pre\n",
    "\n",
    "    pre_combined = list(zip(pre_texts, pre_labels))\n",
    "    random.shuffle(pre_combined)\n",
    "    pre_texts, pre_labels = zip(*pre_combined)\n",
    "\n",
    "    post_pos_texts = positive_texts[n_pos_pre : n_pos_pre + n_pos_post]\n",
    "    post_neg_texts = negative_texts[n_neg_pre : n_neg_pre + n_neg_post]\n",
    "    post_texts = post_pos_texts + post_neg_texts\n",
    "    post_labels = [1] * n_pos_post + [0] * n_neg_post\n",
    "\n",
    "    post_combined = list(zip(post_texts, post_labels))\n",
    "    random.shuffle(post_combined)\n",
    "    post_texts, post_labels = zip(*post_combined)\n",
    "\n",
    "    texts = list(pre_texts) + list(post_texts)\n",
    "    true_labels = list(pre_labels) + list(post_labels)\n",
    "\n",
    "    return texts, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "changepoint = 400\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if \"model\" not in locals() or \"tokenizer\" not in locals():\n",
    "    model, tokenizer = get_pretrained_sentiment_model(device)\n",
    "\n",
    "print(\"Generating mixed sentiment dataset (60%/40% to 40%/60%)...\")\n",
    "texts, true_labels = generate_mixed_sentiment_dataset(length, changepoint)\n",
    "\n",
    "print(\"Getting predictions...\")\n",
    "predictions = []\n",
    "probabilities = []\n",
    "\n",
    "for i, text in enumerate(tqdm(texts)):\n",
    "    pred, prob = predict_sentiment(model, tokenizer, text, device)\n",
    "    predictions.append(pred)\n",
    "    probabilities.append(prob)\n",
    "\n",
    "probabilities = torch.stack(\n",
    "    probabilities\n",
    ")\n",
    "\n",
    "left_score = np.zeros((length, length))\n",
    "seen_sentiments = {0: 0, 1: 0}\n",
    "\n",
    "for t in range(length):\n",
    "    seen_sentiments[predictions[t]] += 1\n",
    "    curr_sentiment = max(seen_sentiments, key=seen_sentiments.get)\n",
    "\n",
    "    for r in range(t + 1):\n",
    "        left_score[t, r] = probabilities[r, curr_sentiment] / (\n",
    "            1 - probabilities[r, curr_sentiment]\n",
    "        )\n",
    "\n",
    "right_score = np.zeros((length, length))\n",
    "seen_sentiments = {0: 0, 1: 0}\n",
    "\n",
    "for i in range(length - 1, -1, -1):\n",
    "    t = length - i - 1\n",
    "    seen_sentiments[predictions[i]] += 1\n",
    "    curr_sentiment = max(seen_sentiments, key=seen_sentiments.get)\n",
    "\n",
    "    right_score[t, t:] = probabilities[t:, curr_sentiment] / (\n",
    "        1 - probabilities[t:, curr_sentiment]\n",
    "    )\n",
    "\n",
    "print(\"Calculating discrepancy scores...\")\n",
    "discrepancy_scores, statistics = get_discrepancy_scores(left_score, right_score, length)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(discrepancy_scores)\n",
    "plt.axvline(x=changepoint, color=\"red\", linestyle=\"--\", label=\"True Changepoint\")\n",
    "plt.xlabel(\"Position t\")\n",
    "plt.ylabel(\"Discrepancy Score\")\n",
    "plt.title(\n",
    "    \"Mixed Sentiment Analysis Changepoint Detection\\n(Pre: 60% pos/40% neg, Post: 40% pos/60% neg)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "p_values_left = np.array([s[0].pvalue for s in statistics])\n",
    "p_values_right = np.array([s[1].pvalue for s in statistics])\n",
    "\n",
    "min_method = chi2.cdf(\n",
    "    np.minimum(2 * p_values_left, 2 * p_values_right, np.ones_like(p_values_left)), 4\n",
    ")\n",
    "threshold = 0.05\n",
    "\n",
    "plt.plot(np.arange(1, length), min_method)\n",
    "plt.axvline(\n",
    "    changepoint,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Changepoint ($\\\\xi = {changepoint}$)\",\n",
    ")\n",
    "plt.axhline(\n",
    "    threshold,\n",
    "    color=\"green\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"Threshold ($\\\\alpha = {threshold}$)\",\n",
    ")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"p-value ($p_t$)\")\n",
    "plt.title(\n",
    "    \"p-values for SST-2 Mixed Sentiment Change\\n(Pre: 60% pos/40% neg, Post: 40% pos/60% neg)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "confidence_set = np.argwhere(min_method > threshold).flatten()\n",
    "confidence_interval = (\n",
    "    (confidence_set[0], confidence_set[-1]) if len(confidence_set) > 0 else None\n",
    ")\n",
    "\n",
    "print(f\"True changepoint: {changepoint}\")\n",
    "print(f\"Confidence interval: {confidence_interval}\")\n",
    "print(f\"Minimum of Fisher's statistic at t={np.argmax(min_method)+1}\")\n",
    "\n",
    "pre_positives = sum(1 for i in range(changepoint + 1) if true_labels[i] == 1)\n",
    "pre_negatives = (changepoint + 1) - pre_positives\n",
    "post_positives = sum(1 for i in range(changepoint + 1, length) if true_labels[i] == 1)\n",
    "post_negatives = (length - changepoint - 1) - post_positives\n",
    "\n",
    "print(\"\\nSentiment distribution in data:\")\n",
    "print(\n",
    "    f\"Pre-change: {pre_positives/(changepoint+1)*100:.1f}% positive, {pre_negatives/(changepoint+1)*100:.1f}% negative\"\n",
    ")\n",
    "print(\n",
    "    f\"Post-change: {post_positives/(length-changepoint-1)*100:.1f}% positive, {post_negatives/(length-changepoint-1)*100:.1f}% negative\"\n",
    ")\n",
    "\n",
    "print(\"\\nExamples before changepoint (60% positive, 40% negative):\")\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(0, changepoint)\n",
    "    print(f'Text {i+1}: \"{texts[idx]}\"')\n",
    "    print(\n",
    "        f\"True label: {'Positive' if true_labels[idx] == 1 else 'Negative'}, Predicted: {'Positive' if predictions[idx] == 1 else 'Negative'}\"\n",
    "    )\n",
    "    print(f\"Confidence: {probabilities[idx][predictions[idx]]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f355bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(min_method > threshold).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, length), min_method)\n",
    "plt.axvline(\n",
    "    changepoint,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Changepoint ($\\\\xi = {changepoint}$)\",\n",
    ")\n",
    "plt.axhline(\n",
    "    threshold,\n",
    "    color=\"green\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"Threshold ($\\\\alpha = {threshold}$)\",\n",
    ")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"p-value ($p_t$)\")\n",
    "plt.title(\n",
    "    \"p-values for SST-2 mixed sentiment change\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.savefig(\"images/sentiment-pvalues-mixed.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
